{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushi15092002/mental-health-workload/blob/main/feature_ext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxFSHqX3fBvv",
        "outputId": "2f75ec3b-341c-4191-aecd-f169798a0736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoXNRLxQk0rt"
      },
      "outputs": [],
      "source": [
        "pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aqtJ977m3tI"
      },
      "outputs": [],
      "source": [
        "pip install -U mne-connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHSmtaA6Xe_y"
      },
      "outputs": [],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYtN_Y4Akx8Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mutual_info_score\n",
        "import mne\n",
        "import numpy as np\n",
        "# import mne_connectivity as conn\n",
        "\n",
        "#import data\n",
        "clean_data_dir = \"/content/drive/My Drive/drdo/cleanData_old/VP001/nback3.fif\"\n",
        "\n",
        "# Load raw EEG data\n",
        "epochs = mne.read_epochs(fname=clean_data_dir,verbose=False)\n",
        "\n",
        "# # Define frequency band of interest\n",
        "# freq_band = [8, 12]\n",
        "\n",
        "# # Compute the analytic signal of the raw data\n",
        "# raw_analytic = mne.preprocessing.compute_current_source_density(raw)\n",
        "\n",
        "# # Compute the Phase Lag Index (PLI)\n",
        "# pli = conn.phase_slope_index(raw_analytic, sfreq=raw.info['sfreq'], mode='fourier', indices=None, n_jobs=1, verbose=None)\n",
        "# print(\"pli \",pli)\n",
        "\n",
        "# # Print the PLI values\n",
        "# print(pli)\n",
        "\n",
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "# # Perform Fourier transform on each epoch\n",
        "# freqs = np.arange(1, 100)  # frequencies of interest\n",
        "# n_cycles = freqs / 2.  # number of cycles\n",
        "# X = mne.time_frequency.tfr_array_morlet(epochs, freqs=freqs, n_cycles=n_cycles, sfreq = epochs.info['sfreq'] )\n",
        "# print(\"x \", X)\n",
        "# # Calculate phase difference between the two signals\n",
        "# sig1 = X[:, 0, :, :]  # first signal\n",
        "# sig2 = X[:, 1, :, :]  # second signal\n",
        "# phase_diff = np.angle(sig1) - np.angle(sig2)\n",
        "\n",
        "# print(\"sig1 \",sig1)\n",
        "# print(\"sig2 \",sig2)\n",
        "# print(\"phase_diff \",phase_diff)\n",
        "\n",
        "# Compute the average phase difference across all epochs\n",
        "# avg_phase_diff = np.mean(phase_diff, axis=-1)\n",
        "\n",
        "# print(\"avg_phase_diff \",avg_phase_diff)\n",
        "\n",
        "# # Compute the slope of the phase difference across frequencies\n",
        "# slope = np.polyfit(freqs, avg_phase_diff, 1)[0]\n",
        "\n",
        "# # Compute the PSI as the average slope across all frequencies\n",
        "# psi = np.mean(slope)\n",
        "\n",
        "# Compute the mean of the phase differences across all epochs\n",
        "# mean_phase_diff = np.mean(np.exp(1j*phase_diff), axis=1)\n",
        "# print(\"mean_phase_diff \", mean_phase_diff)\n",
        "\n",
        "# Compute the phase coherence\n",
        "# coherence = np.abs(mean_phase_diff)\n",
        "# print(\"coherence \", coherence)\n",
        "\n",
        "# Extract the data from the two channels of interest\n",
        "sig1 = epochs.get_data()[:, 0, :]\n",
        "sig2 = epochs.get_data()[:, 1, :]\n",
        "\n",
        "# Compute the mutual information\n",
        "mi = mutual_info_score(sig1.ravel(), sig2.ravel())\n",
        "\n",
        "print('Mutual Information:', mi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzUMqB5KmxpV"
      },
      "outputs": [],
      "source": [
        "pip install pyEDFlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh8GTtSRdy1T"
      },
      "outputs": [],
      "source": [
        "pip install spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owuHCT8ceDgT"
      },
      "outputs": [],
      "source": [
        "pip install nitime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZoEyrdyesCb"
      },
      "outputs": [],
      "source": [
        "pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe9mNp_nqpad"
      },
      "outputs": [],
      "source": [
        "pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PML4k4e-MyOV"
      },
      "outputs": [],
      "source": [
        "pip install pyentrp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHgdv4-SqIY6"
      },
      "outputs": [],
      "source": [
        "pip install nolds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1Ptmy_olfS5"
      },
      "outputs": [],
      "source": [
        "pip install itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUw9GOxZ_W-E"
      },
      "outputs": [],
      "source": [
        "pip install EEGExtract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_0p5fIr-EhH"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pywt\n",
        "import pyedflib\n",
        "import numpy as np\n",
        "import nolds\n",
        "from numpy.random import randn\n",
        "import mne\n",
        "from spectrum import *\n",
        "from os import listdir\n",
        "from nitime import utils\n",
        "import scipy.stats as sp\n",
        "from os.path import isfile, join\n",
        "from nitime.viz import plot_tseries\n",
        "from matplotlib import pyplot as plt\n",
        "from nitime import algorithms as alg\n",
        "from nitime.timeseries import TimeSeries\n",
        "import xlsxwriter\n",
        "from pyentrp import entropy as ent\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "# import EEGExtract as eeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrMKJK3kq1yS"
      },
      "outputs": [],
      "source": [
        "# Create a workbook and add a worksheet.\n",
        "workbook = xlsxwriter.Workbook('Expenses01.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_0 = []\n",
        "features_2 = []\n",
        "features_3 = []\n",
        "features_0.append(names)\n",
        "features_2.append(names)\n",
        "features_3.append(names)\n"
      ],
      "metadata": {
        "id": "fPOuDVwjY0MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_0=np.array(features_0)\n",
        "features_2=np.array(features_2) \n",
        "features_3=np.array(features_3)\n",
        "df=pd.DataFrame(features_0)\n",
        "df.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features0.xlsx',index=False)\n",
        "df2=pd.DataFrame(features_2)\n",
        "df2.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features2.xlsx',index=False)\n",
        "df3=pd.DataFrame(features_3)\n",
        "df3.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features3.xlsx',index=False)"
      ],
      "metadata": {
        "id": "EWsDSbOpeOxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxOkwHSle8VC"
      },
      "outputs": [],
      "source": [
        "clean_data_dir = \"/content/drive/My Drive/drdo/cleanData_new\"\n",
        "read_datasetA(clean_data_dir)\n",
        "  # features_0 = []\n",
        "# features_0 = add_features(clean_data_dir, 3)\n",
        "# features_0=np.array(features_0)\n",
        "# df=pd.DataFrame(features_0)\n",
        "# df.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features0.xlsx',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1YfI6TIkcl6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_datasetA(subjectPath):\n",
        "     \n",
        "    try:\n",
        "        root, dirs, files = next(os.walk(subjectPath))\n",
        "\n",
        "        # features_0 = []\n",
        "        # features_2 = []\n",
        "        # features_3 = []\n",
        "        \n",
        "        \n",
        "        for folder_name in dirs:\n",
        "          path_str = subjectPath + \"/\" + folder_name\n",
        "\n",
        "          if folder_name == \"0_back\":\n",
        "            level = 1\n",
        "          elif folder_name == \"2_back\":\n",
        "            level =2\n",
        "          else:\n",
        "            level = 3\n",
        "\n",
        "          for entry in os.listdir(Path(path_str)):\n",
        "            file_path = path_str + \"/\" + entry\n",
        "            print(\"file_path \", file_path)\n",
        "            print(\"level \",level)\n",
        "            add_features(file_path,level)\n",
        "            # print(\"fearure, \",feature)\n",
        " \n",
        "            # if folder_name == \"0_back\":\n",
        "            #   features_0.append(feature)\n",
        "            # elif folder_name == \"2_back\":\n",
        "            #   features_2.append(feature)\n",
        "            # else:\n",
        "            #   features_3.append(feature)\n",
        "\n",
        "        # features.append(feature)\n",
        "        # features_0=np.array(features_0)\n",
        "        # features_2=np.array(features_2)\n",
        "        # features_3=np.array(features_3)\n",
        "        features_0=np.array(features_0)\n",
        "        features_2=np.array(features_2) \n",
        "        features_3=np.array(features_3)\n",
        "        df=pd.DataFrame(features_0)\n",
        "        df.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features0.xlsx',index=False)\n",
        "        df2=pd.DataFrame(features_2)\n",
        "        df2.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features2.xlsx',index=False)\n",
        "        df3=pd.DataFrame(features_3)\n",
        "        df3.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features3.xlsx',index=False)\n",
        "            \n",
        "    except StopIteration:\n",
        "        pass\n",
        "        print(\"Error ocurred:\")\n",
        "        print(\"Directory with dataset does not found!\")\n",
        "        print(\"Program will be terminated\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeplYMB2-wRm"
      },
      "outputs": [],
      "source": [
        "names = ['Fractal_Dimension','Coeffiecient_of_Variation','Mean_of_Vertex_to_Vertex_Slope','Variance_of_Vertex_to_Vertex_Slope',\n",
        "         'Hjorth_Activity','Hjorth_Mobility','Hjorth_Complexity',\n",
        "         'Kurtosis','2nd_Difference_Mean','2nd_Difference_Max',\n",
        "         'Skewness','1st_Difference_Mean','1st_Difference_Max',\n",
        "         'FFT_Delta_MaxPower','FFT_Theta_MaxPower','FFT_Alpha_MaxPower','FFT_Beta_MaxPower','Delta/Theta','Delta/Alpha','Theta/Alpha','(Delta+Theta)/Alpha',\n",
        "         '1_Wavelet_Approximate_Mean','1_Wavelet_Approximate_Std_Deviation','1_Wavelet_Approximate_Energy','1_Wavelet_Detailed_Mean','1_Wavelet_Detailed_Std_Deviation','1_Wavelet_Detailed_Energy','1_Wavelet_Approximate_Entropy','1_Wavelet_Detailed_Entropy',\n",
        "         '2_Wavelet_Approximate_Mean','2_Wavelet_Approximate_Std_Deviation','2_Wavelet_Approximate_Energy','2_Wavelet_Detailed_Mean','2_Wavelet_Detailed_Std _Deviation','2_Wavelet_Detailed_Energy','2_Wavelet_Approximate_Entropy','2_Wavelet_Detailed_Entropy',\n",
        "         '3_Wavelet_Approximate_Mean','3_Wavelet_Approximate_Std_Deviation','3_Wavelet_Approximate_Energy','3_Wavelet_Detailed_Mean','3_Wavelet_Detailed_Std _Deviation','3_Wavelet_Detailed_Energy','3_Wavelet_Approximate_Entropy','3_Wavelet_Detailed_Entropy',\n",
        "         '4_Wavelet_Approximate_Mean','4_Wavelet_Approximate_Std_Deviation','4_Wavelet_Approximate_Energy','4_Wavelet_Detailed_Mean','4_Wavelet_Detailed_Std _Deviation','4_Wavelet_Detailed_Energy','4_Wavelet_Approximate_Entropy','4_Wavelet_Detailed_Entropy',\n",
        "         '5_Wavelet_Approximate_Mean','5_Wavelet_Approximate_Std_Deviation','5_Wavelet_Approximate_Energy','5_Wavelet_Detailed_Mean','5_Wavelet_Detailed_Std _Deviation','5_Wavelet_Detailed_Energy','5_Wavelet_Approximate_Entropy','5_Wavelet_Detailed_Entropy',\n",
        "         '6_Wavelet_Approximate_Mean','6_Wavelet_Approximate_Std_Deviation','6_Wavelet_Approximate_Energy','6_Wavelet_Detailed_Mean','6_Wavelet_Detailed_Std _Deviation','6_Wavelet_Detailed_Energy','6_Wavelet_Approximate_Entropy','6_Wavelet_Detailed_Entropy',\n",
        "         '7_Wavelet_Approximate_Mean','7_Wavelet_Approximate_Std_Deviation','7_Wavelet_Approximate_Energy','7_Wavelet_Detailed_Mean','7_Wavelet_Detailed_Std _Deviation','7_Wavelet_Detailed_Energy','7_Wavelet_Approximate_Entropy','7_Wavelet_Detailed_Entropy',\n",
        "         '8_Wavelet_Approximate_Mean','8_Wavelet_Approximate_Std_Deviation','8_Wavelet_Approximate_Energy','8_Wavelet_Detailed_Mean','8_Wavelet_Detailed_Std _Deviation','8_Wavelet_Detailed_Energy','8_Wavelet_Approximate_Entropy','8_Wavelet_Detailed_Entropy',\n",
        "         '9_Wavelet_Approximate_Mean','9_Wavelet_Approximate_Std_Deviation','9_Wavelet_Approximate_Energy','9_Wavelet_Detailed_Mean','9_Wavelet_Detailed_Std _Deviation','9_Wavelet_Detailed_Energy','9_Wavelet_Approximate_Entropy','9_Wavelet_Detailed_Entropy',\n",
        "         '10_Wavelet_Approximate_Mean','10_Wavelet_Approximate_Std_Deviation','10_Wavelet_Approximate_Energy','10_Wavelet_Detailed_Mean','10_Wavelet_Detailed_Std _Deviation','10_Wavelet_Detailed_Energy','10_Wavelet_Approximate_Entropy','10_Wavelet_Detailed_Entropy',\n",
        "         '11_Wavelet_Approximate_Mean','11_Wavelet_Approximate_Std_Deviation','11_Wavelet_Approximate_Energy','11_Wavelet_Detailed_Mean','11_Wavelet_Detailed_Std _Deviation','11_Wavelet_Detailed_Energy','11_Wavelet_Approximate_Entropy','11_Wavelet_Detailed_Entropy',\n",
        "         '12_Wavelet_Approximate_Mean','12_Wavelet_Approximate_Std_Deviation','12_Wavelet_Approximate_Energy','12_Wavelet_Detailed_Mean','12_Wavelet_Detailed_Std _Deviation','12_Wavelet_Detailed_Energy','12_Wavelet_Approximate_Entropy','12_Wavelet_Detailed_Entropy',\n",
        "         '13_Wavelet_Approximate_Mean','13_Wavelet_Approximate_Std_Deviation','13_Wavelet_Approximate_Energy','13_Wavelet_Detailed_Mean','13_Wavelet_Detailed_Std _Deviation','13_Wavelet_Detailed_Energy','13_Wavelet_Approximate_Entropy','13_Wavelet_Detailed_Entropy',\n",
        "         '14_Wavelet_Approximate_Mean','14_Wavelet_Approximate_Std_Deviation','14_Wavelet_Approximate_Energy','14_Wavelet_Detailed_Mean','14_Wavelet_Detailed_Std _Deviation','14_Wavelet_Detailed_Energy','14_Wavelet_Approximate_Entropy','14_Wavelet_Detailed_Entropy',\n",
        "         '15_Wavelet_Approximate_Mean','15_Wavelet_Approximate_Std_Deviation','15_Wavelet_Approximate_Energy','15_Wavelet_Detailed_Mean','15_Wavelet_Detailed_Std _Deviation','15_Wavelet_Detailed_Energy','15_Wavelet_Approximate_Entropy','15_Wavelet_Detailed_Entropy',\n",
        "         '16_Wavelet_Approximate_Mean','16_Wavelet_Approximate_Std_Deviation','16_Wavelet_Approximate_Energy','16_Wavelet_Detailed_Mean','16_Wavelet_Detailed_Std _Deviation','16_Wavelet_Detailed_Energy','16_Wavelet_Approximate_Entropy','16_Wavelet_Detailed_Entropy',\n",
        "         '17_Wavelet_Approximate_Mean','17_Wavelet_Approximate_Std_Deviation','17_Wavelet_Approximate_Energy','17_Wavelet_Detailed_Mean','17_Wavelet_Detailed_Std _Deviation','17_Wavelet_Detailed_Energy','17_Wavelet_Approximate_Entropy','17_Wavelet_Detailed_Entropy',\n",
        "         '18_Wavelet_Approximate_Mean','18_Wavelet_Approximate_Std_Deviation','18_Wavelet_Approximate_Energy','18_Wavelet_Detailed_Mean','18_Wavelet_Detailed_Std _Deviation','18_Wavelet_Detailed_Energy','18_Wavelet_Approximate_Entropy','18_Wavelet_Detailed_Entropy',\n",
        "         '19_Wavelet_Approximate_Mean','19_Wavelet_Approximate_Std_Deviation','19_Wavelet_Approximate_Energy','19_Wavelet_Detailed_Mean','19_Wavelet_Detailed_Std _Deviation','19_Wavelet_Detailed_Energy','19_Wavelet_Approximate_Entropy','19_Wavelet_Detailed_Entropy',\n",
        "         '20_Wavelet_Approximate_Mean','20_Wavelet_Approximate_Std_Deviation','20_Wavelet_Approximate_Energy','20_Wavelet_Detailed_Mean','20_Wavelet_Detailed_Std _Deviation','20_Wavelet_Detailed_Energy','20_Wavelet_Approximate_Entropy','20_Wavelet_Detailed_Entropy',\n",
        "         '21_Wavelet_Approximate_Mean','21_Wavelet_Approximate_Std_Deviation','21_Wavelet_Approximate_Energy','21_Wavelet_Detailed_Mean','21_Wavelet_Detailed_Std _Deviation','21_Wavelet_Detailed_Energy','21_Wavelet_Approximate_Entropy','21_Wavelet_Detailed_Entropy',\n",
        "         '22_Wavelet_Approximate_Mean','22_Wavelet_Approximate_Std_Deviation','22_Wavelet_Approximate_Energy','22_Wavelet_Detailed_Mean','22_Wavelet_Detailed_Std _Deviation','22_Wavelet_Detailed_Energy','22_Wavelet_Approximate_Entropy','22_Wavelet_Detailed_Entropy',\n",
        "         '23_Wavelet_Approximate_Mean','23_Wavelet_Approximate_Std_Deviation','23_Wavelet_Approximate_Energy','23_Wavelet_Detailed_Mean','23_Wavelet_Detailed_Std _Deviation','23_Wavelet_Detailed_Energy','23_Wavelet_Approximate_Entropy','23_Wavelet_Detailed_Entropy',\n",
        "         '24_Wavelet_Approximate_Mean','24_Wavelet_Approximate_Std_Deviation','24_Wavelet_Approximate_Energy','24_Wavelet_Detailed_Mean','24_Wavelet_Detailed_Std _Deviation','24_Wavelet_Detailed_Energy','24_Wavelet_Approximate_Entropy','24_Wavelet_Detailed_Entropy',\n",
        "         '25_Wavelet_Approximate_Mean','25_Wavelet_Approximate_Std_Deviation','25_Wavelet_Approximate_Energy','25_Wavelet_Detailed_Mean','25_Wavelet_Detailed_Std _Deviation','25_Wavelet_Detailed_Energy','25_Wavelet_Approximate_Entropy','25_Wavelet_Detailed_Entropy',\n",
        "         '26_Wavelet_Approximate_Mean','26_Wavelet_Approximate_Std_Deviation','26_Wavelet_Approximate_Energy','26_Wavelet_Detailed_Mean','26_Wavelet_Detailed_Std _Deviation','26_Wavelet_Detailed_Energy','26_Wavelet_Approximate_Entropy','26_Wavelet_Detailed_Entropy',\n",
        "         '27_Wavelet_Approximate_Mean','27_Wavelet_Approximate_Std_Deviation','27_Wavelet_Approximate_Energy','27_Wavelet_Detailed_Mean','27_Wavelet_Detailed_Std _Deviation','27_Wavelet_Detailed_Energy','27_Wavelet_Approximate_Entropy','27_Wavelet_Detailed_Entropy',\n",
        "         '28_Wavelet_Approximate_Mean','28_Wavelet_Approximate_Std_Deviation','28_Wavelet_Approximate_Energy','28_Wavelet_Detailed_Mean','28_Wavelet_Detailed_Std _Deviation','28_Wavelet_Detailed_Energy','28_Wavelet_Approximate_Entropy','28_Wavelet_Detailed_Entropy',\n",
        "         '29_Wavelet_Approximate_Mean','29_Wavelet_Approximate_Std_Deviation','29_Wavelet_Approximate_Energy','29_Wavelet_Detailed_Mean','29_Wavelet_Detailed_Std _Deviation','29_Wavelet_Detailed_Energy','29_Wavelet_Approximate_Entropy','29_Wavelet_Detailed_Entropy',\n",
        "         '30_Wavelet_Approximate_Mean','30_Wavelet_Approximate_Std_Deviation','30_Wavelet_Approximate_Energy','30_Wavelet_Detailed_Mean','30_Wavelet_Detailed_Std _Deviation','30_Wavelet_Detailed_Energy','30_Wavelet_Approximate_Entropy','30_Wavelet_Detailed_Entropy',\n",
        "         'Shannon_Entropy', 'Hurst_Exponent', 'Permutation_Entropy','y']\n",
        "        #  ,'detrended fluctuation analysis (DFA)']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yjOOygWkrlJ"
      },
      "outputs": [],
      "source": [
        "def add_features(path, n_back):\n",
        "  epochs = mne.read_epochs(fname=path,verbose=False)\n",
        "  nchans = epochs.info['nchan']\n",
        "  dat = []\n",
        "  for i in epochs:\n",
        "    dat.append(np.array(i))\n",
        "  \n",
        "  dat = np.array(dat)\n",
        "  # print(\"dat.shape\", dat.shape)\n",
        "  # print(\"dat.shape\",dat.shape[2])\n",
        "  tr = dat.shape[1]\n",
        "  # x=[]\n",
        "  # x.append(names)\n",
        "  for i in dat:\n",
        "\n",
        "    features=[]\n",
        "    # Fractal Dimension\n",
        "    features.append(fractal_dimension(i,tr))\n",
        "\n",
        "    #Coeffeicient of Variation\n",
        "    features.append(coeff_var(i,tr))\n",
        "\n",
        "    #Mean of Vertex to Vertex Slope\n",
        "    features.append(slope_mean(i,tr))\n",
        "\n",
        "    #Variance of Vertex to Vertex Slope\n",
        "    features.append(slope_var(i,tr))\n",
        "\n",
        "    #Hjorth Parameters\n",
        "    feature_list = hjorth(i,tr)\n",
        "    for feat in feature_list:\n",
        "        features.append(feat)\n",
        "\n",
        "    #Kurtosis\n",
        "    features.append(kurtosis(i,tr))\n",
        "\n",
        "    #Second Difference Mean\n",
        "    features.append(secDiffMean(i,tr))\n",
        "\n",
        "    #Second Difference Max\n",
        "    features.append(secDiffMax(i,tr))\n",
        "\n",
        "    #Skewness\n",
        "    features.append(skewness(i,tr))\n",
        "\n",
        "    #First Difference Mean\n",
        "    features.append(first_diff_mean(i,tr))\n",
        "\n",
        "    #First Difference Max\n",
        "    features.append(first_diff_max(i,tr))\n",
        "\n",
        "    #FFT Max Power - Delta, Theta, Alpha & Beta Band!\n",
        "    feature_list  =  maxPwelch(i,128,tr)            \n",
        "    for feat in feature_list:\n",
        "      features.append(feat)\n",
        "\n",
        "    #FFT Frequency Ratios\n",
        "    features.append(feature_list[0]/feature_list[1])\n",
        "    features.append(feature_list[0]/feature_list[2])\n",
        "    features.append(feature_list[1]/feature_list[3])\n",
        "    features.append((feature_list[0] + feature_list[1])/feature_list[2])\n",
        "\n",
        "    # wavlent features\n",
        "    feature_list = wavelet_features(i,nchans)\n",
        "    for feat in feature_list:\n",
        "        features.append(feat)\n",
        "\n",
        "    # Shanon Entropy\n",
        "    abc = s_entropy(i,tr)\n",
        "    # print('Shape SE = ',np.shape(abc))\n",
        "    features.append(abc)\n",
        "\n",
        "    # Hurst Exponent\n",
        "    features.append(hurst(i,tr))\n",
        "\n",
        "    # Sample entropy\n",
        "    # features.append(sample_entropy(i))\n",
        "\n",
        "    # Permuation Entropy\n",
        "    features.append(permutation_entropy(i,tr))\n",
        "\n",
        "    # df analysis\n",
        "    # features.append(df_analysis(i,tr))\n",
        "\n",
        "    # coherence\n",
        "    # features.append(coherence(i))\n",
        "\n",
        "    #Spectral Entropy\n",
        "    # a = spectral_entropy(i, 128)\n",
        "    # print('Shape SE = ',np.shape(a))\n",
        "    # features.append(a)\n",
        "    \n",
        "    features.append(n_back)\n",
        "    \n",
        "    # x.append(features)\n",
        "\n",
        "    print(\"n_back   \", n_back)\n",
        "    if n_back == 1:\n",
        "      print(\"inside 1 \")\n",
        "      features_0.append(features)\n",
        "    elif n_back == 2:\n",
        "      features_2.append(features)\n",
        "    else:\n",
        "      features_3.append(features)\n",
        "\n",
        "  # x=np.array(x)      \n",
        "  # return x\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PbbQnN6ghKB"
      },
      "outputs": [],
      "source": [
        "def mean(data):\n",
        "    # print(\"data\")\n",
        "    # print(data)\n",
        "    # print(\"mean\")\n",
        "    # print(np.mean(data,axis=0))\n",
        "    return np.mean(data,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM7-65dL_PLz"
      },
      "source": [
        "Fractal Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4hgoYvS_R-D"
      },
      "outputs": [],
      "source": [
        "def fractal_dimension(Z, tr, threshold=0.9):\n",
        "\n",
        "    # Only for 2d image\n",
        "    assert(len(Z.shape) == 2)\n",
        "\n",
        "    def boxcount(Z, k):\n",
        "        S = np.add.reduceat(\n",
        "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
        "                               np.arange(0, Z.shape[1], k), axis=1)\n",
        "\n",
        "        # We count non-empty (0) and non-full boxes (k*k)\n",
        "        return len(np.where((S > 0) & (S < k*k))[0])\n",
        "\n",
        "\n",
        "    # Transform Z into a binary array\n",
        "    Z = (Z < threshold)\n",
        "\n",
        "    # Minimal dimension of image\n",
        "    p = min(Z.shape)\n",
        "\n",
        "    # Greatest power of 2 less than or equal to p\n",
        "    n = 2**np.floor(np.log(p)/np.log(2))\n",
        "\n",
        "    # Extract the exponent\n",
        "    n = int(np.log(n)/np.log(2))\n",
        "\n",
        "    # Build successive box sizes (from 2**n down to 2**1)\n",
        "    sizes = 2**np.arange(n, 1, -1)\n",
        "\n",
        "    # Actual box counting with decreasing size\n",
        "    counts = []\n",
        "    for size in sizes:\n",
        "        counts.append(boxcount(Z, size))\n",
        "\n",
        "    # Fit the successive log(sizes) with log (counts)\n",
        "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
        "    return -coeffs[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIly8FO_Sxa"
      },
      "source": [
        "coeff of var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHyXcTyu_V4q"
      },
      "outputs": [],
      "source": [
        "def coeff_var(a,tr):\n",
        "    b = a #Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
        "    k = 0; #For counting the current row no.\n",
        "    for i in b:\n",
        "        mean_i = np.mean(i) #Saving the mean of array i\n",
        "        std_i = np.std(i) #Saving the standard deviation of array i\n",
        "        output[k] = std_i/mean_i #computing coefficient of variation\n",
        "        k=k+1\n",
        "    return np.sum(output)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4UVogkM_WjS"
      },
      "source": [
        "mean of vrtex to vertex slope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIA9DJgx_dC7"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "\n",
        "def first_diff(i):\n",
        "    b=i    \n",
        "    \n",
        "    out = np.zeros(len(b))\n",
        "    \n",
        "    for j in range(len(i)):\n",
        "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
        "        \n",
        "        j=j+1\n",
        "        c=out[1:len(out)]\n",
        "    return c #returns first diff\n",
        "\n",
        "\n",
        "def slope_mean(p,tr):\n",
        "    b = p #Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
        "    res = np.zeros(len(b)-1)\n",
        "    \n",
        "    k = 0; #For counting the current row no.\n",
        "    for i in b:\n",
        "        x=i\n",
        "        amp_max = i[argrelextrema(x, np.greater)[0]]\n",
        "        t_max = argrelextrema(x, np.greater)[0]\n",
        "        amp_min = i[argrelextrema(x, np.less)[0]]\n",
        "        t_min = argrelextrema(x, np.less)[0]\n",
        "        t = np.concatenate((t_max,t_min),axis=0)\n",
        "        t.sort()#sort on the basis of time\n",
        "\n",
        "        h=0\n",
        "        amp = np.zeros(len(t))\n",
        "        res = np.zeros(len(t)-1)\n",
        "        for l in range(len(t)):\n",
        "            amp[l]=i[t[l]]\n",
        "           \n",
        "        \n",
        "        amp_diff = first_diff(amp)\n",
        "        \n",
        "        t_diff = first_diff(t)\n",
        "        \n",
        "        for q in range(len(amp_diff)):\n",
        "            res[q] = amp_diff[q]/t_diff[q]         \n",
        "        output[k] = np.mean(res) \n",
        "        k=k+1\n",
        "    return np.sum(output)/tr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfYWDMoz_gwb"
      },
      "source": [
        "variance of vertex to vertex slope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_djuIrV__nyL"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "\n",
        "def first_diff(i):\n",
        "    b=i    \n",
        "    \n",
        "    out = np.zeros(len(b))\n",
        "    \n",
        "    for j in range(len(i)):\n",
        "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
        "        \n",
        "        j=j+1\n",
        "        c=out[1:len(out)]\n",
        "    return c #returns first diff\n",
        "\n",
        "\n",
        "def slope_var(p,tr):\n",
        "    b = p #Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
        "    res = np.zeros(len(b)-1)\n",
        "    \n",
        "    k = 0; #For counting the current row no.\n",
        "    for i in b:\n",
        "        x=i\n",
        "        amp_max = i[argrelextrema(x, np.greater)[0]]#storing maxima value\n",
        "        t_max = argrelextrema(x, np.greater)[0]#storing time for maxima\n",
        "        amp_min = i[argrelextrema(x, np.less)[0]]#storing minima value\n",
        "        t_min = argrelextrema(x, np.less)[0]#storing time for minima value\n",
        "        t = np.concatenate((t_max,t_min),axis=0) #making a single matrix of all matrix\n",
        "        t.sort() #sorting according to time\n",
        "\n",
        "        h=0\n",
        "        amp = np.zeros(len(t))\n",
        "        res = np.zeros(len(t)-1)\n",
        "        for l in range(len(t)):\n",
        "            amp[l]=i[t[l]]\n",
        "           \n",
        "        \n",
        "        amp_diff = first_diff(amp)\n",
        "        \n",
        "        t_diff = first_diff(t)\n",
        "        \n",
        "        for q in range(len(amp_diff)):\n",
        "            res[q] = amp_diff[q]/t_diff[q] #calculating slope        \n",
        "    \n",
        "        output[k] = np.var(res) \n",
        "        k=k+1#counting k\n",
        "    return np.sum(output)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO-q9zoM_oo7"
      },
      "source": [
        "hjorth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E78w0lh6_phz"
      },
      "outputs": [],
      "source": [
        "def hjorth(input,tr):                                             # function for hjorth \n",
        "    realinput = input\n",
        "    hjorth_activity = np.zeros(len(realinput))\n",
        "    hjorth_mobility = np.zeros(len(realinput))\n",
        "    hjorth_diffmobility = np.zeros(len(realinput))\n",
        "    hjorth_complexity = np.zeros(len(realinput))\n",
        "    diff_input = np.diff(realinput)\n",
        "    diff_diffinput = np.diff(diff_input)\n",
        "    k = 0\n",
        "    for j in realinput:\n",
        "        hjorth_activity[k] = np.var(j)\n",
        "        hjorth_mobility[k] = np.sqrt(np.var(diff_input[k])/hjorth_activity[k])\n",
        "        hjorth_diffmobility[k] = np.sqrt(np.var(diff_diffinput[k])/np.var(diff_input[k]))\n",
        "        hjorth_complexity[k] = hjorth_diffmobility[k]/hjorth_mobility[k]\n",
        "        k = k+1\n",
        "    return np.sum(hjorth_activity)/tr, np.sum(hjorth_mobility)/tr, np.sum(hjorth_complexity)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HokjM8do_s6z"
      },
      "source": [
        "kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arJoYtNe_vg7"
      },
      "outputs": [],
      "source": [
        "def kurtosis(a,tr):\n",
        "    b = a # Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
        "    k = 0; # For counting the current row no.\n",
        "    for i in b:\n",
        "        mean_i = np.mean(i) # Saving the mean of array i\n",
        "        std_i = np.std(i) # Saving the standard deviation of array i\n",
        "        t = 0.0\n",
        "        for j in i:\n",
        "            t += (pow((j-mean_i)/std_i,4)-3)\n",
        "        kurtosis_i = t/len(i) # Formula: (1/N)*(summation(x_i-mean)/standard_deviation)^4-3\n",
        "        output[k] = kurtosis_i # Saving the kurtosis in the array created\n",
        "        k +=1 # Updating the current row no.\n",
        "    return np.sum(output)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGNs19gY_war"
      },
      "source": [
        "second difference mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMAOrOv2_0Xb"
      },
      "outputs": [],
      "source": [
        "def secDiffMean(a,tr):\n",
        "    b = a # Extracting the data of the 14 channels\n",
        "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
        "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
        "    k = 0; # For counting the current row no.\n",
        "    for i in b:\n",
        "        t = 0.0\n",
        "        for j in range(len(i)-1):\n",
        "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "        for j in range(len(i)-2):\n",
        "            t += abs(temp1[j+1]-temp1[j]) # Summing the 2nd Diffs\n",
        "        output[k] = t/(len(i)-2) # Calculating the mean of the 2nd Diffs\n",
        "        k +=1 # Updating the current row no.\n",
        "    return np.sum(output)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz8KIZZf_1W8"
      },
      "source": [
        "second difference max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHW5ae2r_5Vd"
      },
      "outputs": [],
      "source": [
        "def secDiffMax(a,tr):\n",
        "    b = a # Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
        "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
        "    k = 0; # For counting the current row no.\n",
        "    t = 0.0\n",
        "    for i in b:\n",
        "        for j in range(len(i)-1):\n",
        "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "        t = temp1[1] - temp1[0]\n",
        "        for j in range(len(i)-2):\n",
        "            if abs(temp1[j+1]-temp1[j]) > t :\n",
        "                t = temp1[j+1]-temp1[j] # Comparing current Diff with the last updated Diff Max\n",
        "\n",
        "        output[k] = t # Storing the 2nd Diff Max for channel k\n",
        "        k +=1 # Updating the current row no.\n",
        "    return np.sum(output)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flR76sKi_53E"
      },
      "source": [
        "skewness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvM9mpz-_9SU"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as sp\n",
        "def skewness(arr,tr):\n",
        "    data = arr \n",
        "    skew_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
        "    index = 0; #current cell position in the output array\n",
        "   \n",
        "    for i in data:\n",
        "        skew_array[index]=sp.stats.skew(i,axis=0,bias=True)\n",
        "        index+=1 #updating the cell position\n",
        "    return np.sum(skew_array)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHqi7ymt_-Ad"
      },
      "source": [
        "first difference mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHpPbvnOABaM"
      },
      "outputs": [],
      "source": [
        "def first_diff_mean(arr,tr):\n",
        "    data = arr \n",
        "    diff_mean_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
        "    index = 0; #current cell position in the output array\n",
        "   \n",
        "    for i in data:\n",
        "        sum=0.0#initializing the sum at the start of each iteration\n",
        "        for j in range(len(i)-1):\n",
        "            sum += abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "           \n",
        "        diff_mean_array[index]=sum/(len(i)-1)\n",
        "        index+=1 #updating the cell position\n",
        "    return np.sum(diff_mean_array)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFV8Xh-LACcM"
      },
      "source": [
        "first difference max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLjl-ZLJALK0"
      },
      "outputs": [],
      "source": [
        "def first_diff_max(arr,tr):\n",
        "    data = arr \n",
        "    diff_max_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
        "    first_diff = np.zeros(len(data[0])-1)#Initialinling the array as all 0s \n",
        "    index = 0; #current cell position in the output array\n",
        "   \n",
        "    for i in data:\n",
        "        max=0.0#initializing at the start of each iteration\n",
        "        for j in range(len(i)-1):\n",
        "            first_diff[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "            if first_diff[j]>max: \n",
        "                max=first_diff[j] # finding the maximum of the first differences\n",
        "        diff_max_array[index]=max\n",
        "        index+=1 #updating the cell position\n",
        "    return np.sum(diff_max_array)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSGTzPS2ANhc"
      },
      "source": [
        "Wavelet Fetures! : \n",
        "Approx Mean, Approx Std Deviation, Approx Energy, Detailed Mean, Detailed Std Deviation, Detailed Energy, Approx Entropy & Detailed Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otz5OfnlAQeb"
      },
      "outputs": [],
      "source": [
        "import pywt\n",
        "\n",
        "def wavelet_features(epoch,channels):\n",
        "    cA_values = []\n",
        "    cD_values = []\n",
        "    cA_mean = []\n",
        "    cA_std = []\n",
        "    cA_Energy =[]\n",
        "    cD_mean = []\n",
        "    cD_std = []\n",
        "    cD_Energy = []\n",
        "    Entropy_D = [] \n",
        "    Entropy_A = []\n",
        "    wfeatures = []\n",
        "    for i in range(channels):\n",
        "        cA,cD=pywt.dwt(epoch[i,:],'coif1')\n",
        "        cA_values.append(cA)\n",
        "        cD_values.append(cD)\t\t#calculating the coefficients of wavelet transform.\n",
        "    for x in range(channels):   \n",
        "        cA_mean.append(np.mean(cA_values[x]))\n",
        "        wfeatures.append(np.mean(cA_values[x]))\n",
        "        \n",
        "        cA_std.append(abs(np.std(cA_values[x])))\n",
        "        wfeatures.append(abs(np.std(cA_values[x])))\n",
        "        \n",
        "        cA_Energy.append(abs(np.sum(np.square(cA_values[x]))))\n",
        "        wfeatures.append(abs(np.sum(np.square(cA_values[x]))))\n",
        "        \n",
        "        cD_mean.append(np.mean(cD_values[x]))\t\t# mean and standard deviation values of coefficents of each channel is stored .\n",
        "        wfeatures.append(np.mean(cD_values[x]))\n",
        "\n",
        "        cD_std.append(abs(np.std(cD_values[x])))\n",
        "        wfeatures.append(abs(np.std(cD_values[x])))\n",
        "        \n",
        "        cD_Energy.append(abs(np.sum(np.square(cD_values[x]))))\n",
        "        wfeatures.append(abs(np.sum(np.square(cD_values[x]))))\n",
        "        \n",
        "        Entropy_D.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
        "        wfeatures.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
        "        \n",
        "        Entropy_A.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x]))))) \n",
        "        wfeatures.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x])))))\n",
        "    return wfeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plwYS9tE1Oo"
      },
      "source": [
        "FFT Max Power - Delta, Theta, Alpha & Beta Band!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7OvEtHtE13_"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "\n",
        "def maxPwelch(data_win,Fs,tr):\n",
        " \n",
        "    \n",
        "    BandF = [0.1, 3, 7, 12, 30]\n",
        "    PMax = np.zeros([tr,(len(BandF)-1)]);\n",
        "    \n",
        "    for j in range(14):\n",
        "        f,Psd = signal.welch(data_win[j,:], Fs)\n",
        "        \n",
        "        for i in range(len(BandF)-1):\n",
        "            fr = np.where((f>BandF[i]) & (f<=BandF[i+1]))\n",
        "            PMax[j,i] = np.max(Psd[fr])\n",
        "    \n",
        "    return np.sum(PMax[:,0])/tr,np.sum(PMax[:,1])/tr,np.sum(PMax[:,2])/tr,np.sum(PMax[:,3])/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD40cOj_rwVG"
      },
      "source": [
        "hurst exponent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK-n4KexPR5G"
      },
      "outputs": [],
      "source": [
        "def hurst(epoch,tr):\n",
        "    hurst = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      H =  nolds.hurst_rs(X)\n",
        "      hurst[index] = H\n",
        "    return np.sum(hurst)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMfS3qlAPSg0"
      },
      "source": [
        "sample entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX9YFlzNrrpv"
      },
      "outputs": [],
      "source": [
        "def sample_entropy(epoch,tr):\n",
        "    sam_en = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      S =  nolds.sampen(X)\n",
        "      sam_en[index] = S\n",
        "      index = index +1\n",
        "    return np.sum(sam_en)/tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRwDrOR4VFdX"
      },
      "outputs": [],
      "source": [
        "def permutation_entropy(epoch,tr):\n",
        "    per_en = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      S =  ent.permutation_entropy(X, 4, 1)\n",
        "      per_en[index] = S\n",
        "      index = index +1\n",
        "    return np.sum(per_en)/tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dNyuxEFF73N"
      },
      "source": [
        "Shanon Entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1bSPBZ6GEjV"
      },
      "outputs": [],
      "source": [
        "def s_entropy(epochs,tr): \n",
        "  # Shanon Entropy\n",
        "  \"\"\" Computes entropy of 0-1 vector. \"\"\"\n",
        "  sh_entropies = np.zeros(len(epochs))\n",
        "  index =0\n",
        "  for labels in epochs:\n",
        "    n_labels = len(labels)\n",
        "    labels = list(labels)\n",
        "    counts = np.bincount(labels)\n",
        "    probs = counts[np.nonzero(counts)] / n_labels\n",
        "    n_classes = len(probs)\n",
        "    if n_classes <= 1:\n",
        "      sh_entropy = 0\n",
        "    else:\n",
        "      sh_entropy =  - np.sum(probs * np.log(probs)) / np.log(n_classes)\n",
        "    sh_entropies[index] = sh_entropy\n",
        "    index = index +1\n",
        "  return np.sum(sh_entropies)/tr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAjsoEBoGFP-"
      },
      "source": [
        "Spectral Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzBkHPtzGNS-"
      },
      "outputs": [],
      "source": [
        "from numpy.fft import fft\n",
        "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum, ones, log2, std\n",
        "from numpy.linalg import svd, lstsq\n",
        "import time\n",
        "\n",
        "def bin_power(X,Band,Fs):\n",
        "    C = fft(X)\n",
        "    C = abs(C)\n",
        "    Power =zeros(len(Band)-1);\n",
        "    for Freq_Index in range(0,len(Band)-1):\n",
        "        Freq = float(Band[Freq_Index])   ## Xin Liu\n",
        "        Next_Freq = float(Band[Freq_Index+1])\n",
        "        print(\"\")\n",
        "        Power[Freq_Index] = sum(C[floor(Freq/Fs*len(X)):floor(Next_Freq/Fs*len(X))])\n",
        "    Power_Ratio = Power/sum(Power)\n",
        "    return Power, Power_Ratio\n",
        "\n",
        "\n",
        "def spectral_entropy(X, Fs, Power_Ratio = None):\n",
        "    \n",
        "    Band = [0.1, 3, 7, 12, 30]\n",
        "    if Power_Ratio is None:\n",
        "        Power, Power_Ratio = bin_power(X, Band, Fs)\n",
        "\n",
        "    Spectral_Entropy = 0\n",
        "    for i in range(0, len(Power_Ratio) - 1):\n",
        "        Spectral_Entropy += Power_Ratio[i] * log(Power_Ratio[i])\n",
        "    Spectral_Entropy /= log(len(Power_Ratio))     # to save time, minus one is omitted\n",
        "    print('Shape of Spectral Entropy = ',n.shape(Spectral_Entropy))\n",
        "    return -1 * Spectral_Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbRWgOPMlCte"
      },
      "source": [
        "Coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl8KGJQllD9V"
      },
      "outputs": [],
      "source": [
        "coh_res = []\n",
        "def coherence(eegData):\n",
        "  # for eegData in epoch:\n",
        "    coh_res = []\n",
        "    print(\"eegData.shape[0] >>>>>>\",eegData.shape[0])\n",
        "    # for ii, jj in itertools.combinations(range(eegData.shape[0]), 2):\n",
        "    #   coh_res.append(CoherenceDelta(eegData, ii, jj, fs=128))\n",
        "\n",
        "    # coh_res = np.array(coh_res)\n",
        "    # print(\"coh_res >>>>>>>>>>>>.\",coh_res)\n",
        "    return coh_res\n",
        "\n",
        "# Coherence in the Delta Band\n",
        "def CoherenceDelta(eegData, i, j, fs=128):\n",
        "    print(\"eegData >>>>>>>>>>\",eegData)\n",
        "    print(\"ii >>>\",i)\n",
        "    print(\"jj >>>\",j)\n",
        "    nfft=eegData.shape[1]\n",
        "    print(\"nfft >>\",nfft)\n",
        "    f, Cxy = signal.coherence(eegData[i,:], eegData[j,:], fs=fs, nfft=nfft, axis=0)#, window=np.hanning(nfft))\n",
        "    out = np.mean(Cxy[np.all([f >= 0.5, f<=4], axis=0)], axis=0)\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaGQBCteOUsR"
      },
      "source": [
        "phase coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb4nGdO7OWyf"
      },
      "outputs": [],
      "source": [
        "def PhaseCoherence(freq, timeSeries, FS):\n",
        "\n",
        "    # Get parameters of input data\n",
        "    nMeasures\t = np.shape(timeSeries)[0]\n",
        "    nSamples \t= np.shape(timeSeries)[1]\n",
        "    nSecs = nSamples / FS\n",
        "    print('Number of measurements =', nMeasures)\n",
        "    print('Number of time samples =', nSamples, '=', nSecs, 'seconds')\n",
        "    \n",
        "    # Calculate FFT for each measurement (spect is freq x measurements)\n",
        "    spect = np.fft.fft(timeSeries, axis=1)\n",
        "    \n",
        "    # Normalise by amplitude\n",
        "    spect = spect / abs(spect)\n",
        "    \n",
        "    # Find spectrum values for frequency bin of interest\n",
        "    freqRes = 1 / nSecs;\n",
        "    foibin = round(freq / freqRes + 1) - 1\n",
        "    spectFoi = spect[:,foibin]\n",
        "    \n",
        "    # Find individual phase angles per measurement at frequency of interest\n",
        "    anglesFoi = np.arctan2(spectFoi.imag, spectFoi.real)\n",
        "    \n",
        "    # PC is root mean square of the sums of the cosines and sines of the angles\n",
        "    PC = np.sqrt((np.sum(np.cos(anglesFoi)))**2 + (np.sum(np.sin(anglesFoi)))**2) / np.shape(anglesFoi)[0]\n",
        "    \n",
        "    # Print the value\n",
        "    print('----------------------------------');\n",
        "    print('Phase coherence value = ' + str(\"{0:.3f}\".format(PC)));\n",
        "        \n",
        "    return PC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8krPoes2nd"
      },
      "source": [
        "Lyapunov exponents "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5unNk0ws3Wl"
      },
      "outputs": [],
      "source": [
        "def LLE(x, tau, n, T, fs):\n",
        "    \"\"\"Calculate largest Lyauponov exponent of a given time series x using\n",
        "    Rosenstein algorithm.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x\n",
        "        list\n",
        "        a time series\n",
        "    n\n",
        "        integer\n",
        "        embedding dimension\n",
        "    tau\n",
        "        integer\n",
        "        Embedding lag\n",
        "    fs\n",
        "        integer\n",
        "        Sampling frequency\n",
        "    T\n",
        "        integer\n",
        "        Mean period\n",
        "    Returns\n",
        "    ----------\n",
        "    Lexp\n",
        "       float\n",
        "       Largest Lyapunov Exponent\n",
        "    Notes\n",
        "    ----------\n",
        "    A n-dimensional trajectory is first reconstructed from the observed data by\n",
        "    use of embedding delay of tau, using pyeeg function, embed_seq(x, tau, n).\n",
        "    Algorithm then searches for nearest neighbour of each point on the\n",
        "    reconstructed trajectory; temporal separation of nearest neighbours must be\n",
        "    greater than mean period of the time series: the mean period can be\n",
        "    estimated as the reciprocal of the mean frequency in power spectrum\n",
        "    Each pair of nearest neighbours is assumed to diverge exponentially at a\n",
        "    rate given by largest Lyapunov exponent. Now having a collection of\n",
        "    neighbours, a least square fit to the average exponential divergence is\n",
        "    calculated. The slope of this line gives an accurate estimate of the\n",
        "    largest Lyapunov exponent.\n",
        "    References\n",
        "    ----------\n",
        "    Rosenstein, Michael T., James J. Collins, and Carlo J. De Luca. \"A\n",
        "    practical method for calculating largest Lyapunov exponents from small data\n",
        "    sets.\" Physica D: Nonlinear Phenomena 65.1 (1993): 117-134.\n",
        "    Examples\n",
        "    ----------\n",
        "    >>> import pyeeg\n",
        "    >>> X = numpy.array([3,4,1,2,4,51,4,32,24,12,3,45])\n",
        "    >>> pyeeg.LLE(X,2,4,1,1)\n",
        "       0.18771136179353307\n",
        "    \"\"\"\n",
        "\n",
        "    from embedded_sequence import embed_seq\n",
        "\n",
        "    Em = embed_seq(x, tau, n)\n",
        "    M = len(Em)\n",
        "    A = numpy.tile(Em, (len(Em), 1, 1))\n",
        "    B = numpy.transpose(A, [1, 0, 2])\n",
        "\n",
        "    #  square_dists[i,j,k] = (Em[i][k]-Em[j][k])^2\n",
        "    square_dists = (A - B) ** 2\n",
        "\n",
        "    #  D[i,j] = ||Em[i]-Em[j]||_2\n",
        "    D = numpy.sqrt(square_dists[:, :, :].sum(axis=2))\n",
        "\n",
        "    # Exclude elements within T of the diagonal\n",
        "    band = numpy.tri(D.shape[0], k=T) - numpy.tri(D.shape[0], k=-T - 1)\n",
        "    band[band == 1] = numpy.inf\n",
        "\n",
        "    # nearest neighbors more than T steps away\n",
        "    neighbors = (D + band).argmin(axis=0)\n",
        "\n",
        "    # in_bounds[i,j] = (i+j <= M-1 and i+neighbors[j] <= M-1)\n",
        "    inc = numpy.tile(numpy.arange(M), (M, 1))\n",
        "    row_inds = (numpy.tile(numpy.arange(M), (M, 1)).T + inc)\n",
        "    col_inds = (numpy.tile(neighbors, (M, 1)) + inc.T)\n",
        "    in_bounds = numpy.logical_and(row_inds <= M - 1, col_inds <= M - 1)\n",
        "\n",
        "    # Uncomment for old (miscounted) version\n",
        "    # in_bounds = numpy.logical_and(row_inds < M - 1, col_inds < M - 1)\n",
        "    row_inds[~in_bounds] = 0\n",
        "    col_inds[~in_bounds] = 0\n",
        "\n",
        "    # neighbor_dists[i,j] = ||Em[i+j]-Em[i+neighbors[j]]||_2\n",
        "    neighbor_dists = numpy.ma.MaskedArray(D[row_inds, col_inds], ~in_bounds)\n",
        "\n",
        "    #  number of in-bounds indices by row\n",
        "    J = (~neighbor_dists.mask).sum(axis=1)\n",
        "\n",
        "    # Set invalid (zero) values to 1; log(1) = 0 so sum is unchanged\n",
        "    neighbor_dists[neighbor_dists == 0] = 1\n",
        "    d_ij = numpy.sum(numpy.log(neighbor_dists.data), axis=1)\n",
        "    mean_d = d_ij[J > 0] / J[J > 0]\n",
        "\n",
        "    x = numpy.arange(len(mean_d))\n",
        "    X = numpy.vstack((x, numpy.ones(len(mean_d)))).T\n",
        "    [m, c] = numpy.linalg.lstsq(X, mean_d)[0]\n",
        "    Lexp = fs * m\n",
        "    return Lexp\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}